\documentclass{article}
\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}

\section*{Algorithm: Outlier/Cluster Detection with Topic Distributions}

\begin{algorithm}
\caption{Outlier/Cluster Detection Algorithm for Bank 10-K Items}
\begin{algorithmic}[1]
\REQUIRE Text data for Item $i$ of bank $j$ over years $t = 1, \dots, T$.
\ENSURE Continuous outlier and cluster scores normalized by holding company asset size.

\FORALL{years $t \in \{1, \dots, T\}$}
    \FORALL{banks $j$}
        \STATE Extract text $I_{i,j,t}$ for Item $i$ of bank $j$ at year $t$.
        \STATE Split $I_{i,j,t}$ into semantic chunks or paragraphs.
        \STATE Clean $I_{i,j,t}$ by removing stopwords and performing preprocessing.
        \STATE Build topic model using BERTopic:
        \begin{enumerate}
            \item Generate topics $k$ and embeddings $v_{k}$ for each chunk.
            \item Apply hierarchical clustering to group topics into parent topics.
            \item Select parent topics based on hierarchical levels or topic coverage.
        \end{enumerate}
        \STATE Compute topic distributions $D_{i,j,t}$ as percentages:
        \begin{enumerate}
            \item Normalize counts of topics by total topic occurrences in year $t$.
            \item Optionally compute weighted topic distributions based on order of topics discussed.
        \end{enumerate}
    \ENDFOR
\ENDFOR

\STATE Aggregate topic distributions $D_{i,j,t}$ over all years $t$ for each bank $j$.

\STATE Compute embedding matrix $V_{i,j}$ using aggregated topic distributions.

\STATE Apply clustering and outlier detection:
\begin{enumerate}
    \item Use KMeans clustering to determine optimal clusters $k$ using silhouette score.
    \item Test for multivariate normality of $V_{i,j}$:
    \begin{itemize}
        \item \textbf{If normality is True:}
        \begin{enumerate}
            \item Apply robust Mahalanobis distance using Minimum Covariance Determinant (MCD).
            \item Apply Local Outlier Factor (LOF).
            \item Flag points as outliers if identified by both Mahalanobis distance and LOF.
        \end{enumerate}
        \item \textbf{Else:}
        \begin{enumerate}
            \item Apply only LOF for outlier detection.
        \end{enumerate}
    \end{itemize}
\end{enumerate}

\STATE Convert discrete outlier and cluster outputs into continuous scores.
\STATE Normalize outlier and cluster scores by holding company asset size.

\RETURN Continuous outlier and cluster scores.

\end{algorithmic}
\end{algorithm}

\end{document}






\documentclass[11pt]{article}
\usepackage{algorithm2e}
\usepackage{amsmath}

\begin{document}

\section*{Algorithm: Topic Distribution and Outlier Detection}

\begin{algorithm}[H]
\SetAlgoLined
\KwIn{
    SEC Filing Item \(i\) (e.g., "Item 1A: Risk Factors"), 
    Financial Institution \(j\), 
    Reporting Years \(t = 1, 2, \ldots, T\)
}
\KwOut{Clustered topics and detected outliers for Item \(i\) for each institution \(j\)}

\For{each year \(t\)}{
    Extract text \(I_{i,j,t}\) for Item \(i\) of institution \(j\) at time \(t\)\;
    Split \(I_{i,j,t}\) into semantic chunks using a semantic chunker\;
    Build a topic model \(T\) using BERTopic\;
    Run hierarchical clustering on \(T\) to reduce the dimensionality of topics\;
    Create topic distributions \(D_{i,j,t}\) (as percentages, not counts) per chunk\;
}

\For{each institution \(j\)}{
    Aggregate \(D_{i,j,t}\) across years to create temporal vectors\;
    Standardize distributions using a robust scaler\;
    Apply PCA to reduce dimensionality while retaining 95\% variance\;
    Apply KMeans clustering to determine \(k\), the optimal number of clusters, using silhouette scores\;
    Test for multivariate normality of the PCA-reduced vectors\;
    
    \eIf{normality is \textbf{True}}{
        Apply robust Mahalanobis distance using Minimum Covariance Determinant (MCD)\;
        Apply Local Outlier Factor (LOF)\;
        Flag outliers if identified by more than one method\;
    }{
        Apply only LOF for outlier detection\;
    }
}

\If{the topic order is relevant (e.g., Risk Factors)}{
    Create weighted topic distributions \(W_{i,j,t}\) based on the order of discussion\;
}

Normalize final scores to account for holding company size (e.g., total assets)\;

Convert outlier detection and clustering results into continuous scores for risk assessment\;

\caption{Topic Distribution and Outlier Detection Algorithm}
\end{algorithm}

\end{document}
